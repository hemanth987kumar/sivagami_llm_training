import os
import torch
import sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# -----------------------------------------------------
# ğŸ”§ Auto-setup defaults (edit these if needed)
# -----------------------------------------------------
DEFAULT_BASE = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
DEFAULT_ADAPTER = "./out-lora"
DEFAULT_OUT = "./merged-model"

# Use env vars if defined
BASE_MODEL = os.environ.get("BASE_MODEL", DEFAULT_BASE)
ADAPTER_DIR = os.environ.get("ADAPTER_DIR", DEFAULT_ADAPTER)
OUT_DIR = os.environ.get("OUT_DIR", DEFAULT_OUT)

# -----------------------------------------------------
# ğŸ§  Helper: colored print
# -----------------------------------------------------
def log(msg, color="white"):
    colors = {
        "green": "\033[92m", "yellow": "\033[93m",
        "red": "\033[91m", "cyan": "\033[96m", "reset": "\033[0m"
    }
    print(colors.get(color, ""), msg, colors["reset"], sep="")

# -----------------------------------------------------
# ğŸ§© Pre-flight checks
# -----------------------------------------------------
log("ğŸ” Starting LoRA â†’ Base merge process...", "cyan")

# Convert paths
adapter_path = Path(ADAPTER_DIR).resolve()
out_path = Path(OUT_DIR).resolve()

# 1ï¸âƒ£ Base model sanity
log(f"ğŸ“¦ Base model: {BASE_MODEL}", "yellow")

# 2ï¸âƒ£ Adapter folder validation
if not adapter_path.exists():
    log(f"âŒ Adapter folder not found: {adapter_path}", "red")
    log("ğŸ’¡ Run your QLoRA training first (trainer.save_model(out-lora))", "yellow")
    sys.exit(1)

if not any(adapter_path.glob("*")):
    log(f"âŒ Adapter folder exists but empty: {adapter_path}", "red")
    sys.exit(1)
else:
    log(f"âœ… Found adapter weights in {adapter_path}", "green")

# 3ï¸âƒ£ Output directory
out_path.mkdir(parents=True, exist_ok=True)
log(f"ğŸ“‚ Output will be saved to: {out_path}", "yellow")

# 4ï¸âƒ£ GPU / CPU check
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if torch.cuda.is_available() else torch.float32
log(f"ğŸ–¥ï¸  Using device: {device.upper()} ({'GPU' if torch.cuda.is_available() else 'CPU'})", "cyan")

# -----------------------------------------------------
# ğŸš€ Load base model + adapter
# -----------------------------------------------------
try:
    base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto" if torch.cuda.is_available() else "cpu",
        torch_dtype=dtype,
        trust_remote_code=True,
    )
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
    log("âœ… Loaded base model successfully.", "green")
except Exception as e:
    log(f"âŒ Failed to load base model: {e}", "red")
    sys.exit(1)

# -----------------------------------------------------
# ğŸ”§ Merge adapter into base
# -----------------------------------------------------
try:
    log("ğŸ”— Loading LoRA adapter...", "cyan")
    peft_model = PeftModel.from_pretrained(base, str(adapter_path))
    log("ğŸ”„ Merging LoRA weights into base...", "cyan")
    merged = peft_model.merge_and_unload()
except Exception as e:
    log(f"âŒ Merge failed: {e}", "red")
    sys.exit(1)

# -----------------------------------------------------
# ğŸ’¾ Save merged model
# -----------------------------------------------------
try:
    merged.save_pretrained(str(out_path), safe_serialization=True)
    tok.save_pretrained(str(out_path))
    log(f"âœ… Merge completed successfully!", "green")
    log(f"ğŸ“ Merged model saved to: {out_path}", "cyan")
except Exception as e:
    log(f"âŒ Failed to save merged model: {e}", "red")
    sys.exit(1)

# -----------------------------------------------------
# ğŸ§ª Quick verification
# -----------------------------------------------------
try:
    test_model = AutoModelForCausalLM.from_pretrained(str(out_path), device_map="cpu", torch_dtype=torch.float32)
    _ = test_model.generate
    log("ğŸ§ª Verification: merged model loaded successfully. âœ…", "green")
except Exception as e:
    log(f"âš ï¸  Verification failed: {e}", "yellow")

log("ğŸ¯ Done!", "green")
